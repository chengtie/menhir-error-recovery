(** This example parser illustrates how error recovery can be
   programmed using the incremental parsers generated by Menhir.
   There are many ways to recover from parsing errors and they often
   are specific to the grammar we are parsing. Here is our strategy.
   From the grammar, we know that a command is probably ended by a
   semicolon or followed right brace if the semicolon is
   absent. Therefore, if there is a parse error in a command, we can
   skip all the tokens until we consumed the ending semicolon or until
   we are about to consume the right brace. After that we are ready to
   continue parsing, either by closing a block or by moving to another
   command.
   The problem is that we want to continue parsing from a
   syntactically valid state of the parser: a parser which is in the
   middle of a syntactically-ill command is not in such a state. For
   this reason, we maintain the last state of the parser which
   successfully recognized a full command or a full definition: we
   always recover parsing from this state, as if the syntactically
   invalid subsequence of tokens never existed.
   This strategy can be refined to avoid skipping too large part of
   the input.
   Maintaining several coexisting state of the parser is free with the
   incremental mode of Menhir because parsers are purely
   functional. For the lexer generated with ocamllex, it is a bit less
   direct. See {!PureLexer}.
*)

open Parser
open Parser.Incremental
open Parser.MenhirInterpreter
open Lexing
open PureLexer
open ParseError

(**
   [last_reduction] is the last state of the parser which successfully
   [`FoundDefinitionAt] some checkpoint, [`FoundCommandAt] some checkpoint
   or, at the beginning of the input, [`FoundNothingAt] some checkpoint.
   Depending on the context, we decide to skip the tokens which correspond
   to the next (syntactically invalid) command or definition. For this we
   use an heuristic which seems to work in practice:
   - If the previously recognized nonterminal was a command, we skip the
     tokens until we consumed a semicolon or we are just before a right brace.
   - If the previously recognized nonterminal was a definition, we
     skip the tokens until the next DEF or VAR, that is to what is likely
     to start a new definition.
*)
let resume_on_error last_reduction (lex : LexerF.t) currentStateNumber positions env: LexerF.t * AST.expression checkpoint =
  match currentStateNumber with | _ -> ();
  match last_reduction with
  | `FoundCommandAt checkpoint ->
     let lex =
       LexerF.skip_until_before (fun t -> t = SEMICOLON || t = RBRACE) lex
     in
     let lex =
       if LexerF.get' lex = SEMICOLON then snd (LexerF.next lex) else lex
     in
     (lex, checkpoint)
  | (`FoundNothingAt checkpoint | `FoundDefinitionAt checkpoint) ->
     (LexerF.skip_until_before
        (function EOF | DEF | VAR -> true | _ -> false)
        lex,
      checkpoint)
  | `FoundExpressionAt checkpoint ->
     let (startp, endp) = positions in
     match currentStateNumber with
     | Some 14 ->
       (* element item: 3: an expression -> an expression + .an expression *)
       let env_new = feed (T T_FAKEEXPRESSION) startp () endp env in
       Printf.printf "BEFORE:\n";
       print_env env;
       Printf.printf "\nAFTER:\n";
       print_env env_new;
       (lex, input_needed env_new)
     | _ -> (
       (* for '(1' or '(1+2' or '(1+2*3' or '((1+2)' we add ')': *)
       let acceptable_me checkpoint token pos =
         let triple = (token, pos, pos) in
         let checkpoint = offer checkpoint triple in
         match shifts checkpoint with
         | None -> (false, None)
         | Some _env -> (true, Some _env)
       in
       match acceptable_me checkpoint RPAREN endp with
       | (xxx, Some _env) when xxx ->
         Printf.printf "Can amend";
         let env_new = feed (T T_FAKERPAREN) startp () endp _env in
         (lex, input_needed env_new)
       | _ -> 
         if LexerF.get' lex = RPAREN
         (* for extra closing parenthesis: *) 
         then (lex, checkpoint)
         else failwith "Other cases")
      
(** This function updates the last fully correct state of the parser. *)
let update_last_reduction checkpoint production last_reduction =
  match lhs production with
  (* | X (N N_command) ->
     `FoundCommandAt checkpoint
  | X (N N_definition) ->
     `FoundDefinitionAt checkpoint *)
  | X (N N_expression) ->
     (* Printf.printf "update_last_reduction N N_expression\n";
     Printf.printf "%s\n" (Symbol.string_of_production production); *)
     `FoundExpressionAt checkpoint
  | _ ->
     last_reduction

let parse lexbuf =
  LexerF.initialize lexbuf;

  let rec on_error last_reduction (lexer : LexerF.t) (checkpoint : AST.expression checkpoint) =
    contextual_error_msg lexer checkpoint (fun currentStateNumber positions ->
      resume_on_error last_reduction lexer currentStateNumber positions
    )

  (* [run] is the loop function of the parser.
      We maintain [last_reduction] as seen earlier but we also
      save [input_needed] which is the last state of the automaton
      that asked for a token. Since we can change the next token
      observe by this state when we skip tokens, it is the right state from
      which a recovering can be triggered.
      [lexer] and [checkpoint] are the (purely functional) state of
      the lexer and the parser respectively.
   *)
  and run last_reduction (input_needed : AST.expression checkpoint) (lexer : LexerF.t) (checkpoint : AST.expression checkpoint) =
    match checkpoint with
    | InputNeeded _ ->
       Printf.printf "InputNeeded, env always as before\n";
       (* print_env env; *)
       let token, lexer = LexerF.next lexer in
       (* Notice that we update [input_needed] here. *)
       run last_reduction checkpoint lexer (offer checkpoint token)
    | Accepted x ->
       (* We will always return a semantic value. *)
       Printf.printf "AST:\n%s\n" (AST.show_expression x);
       x
    | Rejected
    | HandlingError _ ->
       (* [on_error] is responsible for recovering from the parsing
          error by returning a lexer state and a parser state that can
          work together to complete the analysis if the suffix of the
          input is syntactically correct. *)
       let lexer, after_error = on_error last_reduction lexer input_needed in
       Printf.printf "\nafter_error\n";
       run last_reduction input_needed lexer after_error
    | Shifting (_, _, _) ->
       Printf.printf "\nShifting\n";
       (* Printf.printf "Before:\n";
       print_env env_before;
       Printf.printf "After:\n";
       print_env env_after; *)
       (* Nothing special here, we simply resume parsing. *)
       run last_reduction input_needed lexer (resume checkpoint)
    | AboutToReduce (_, production) ->
      Printf.printf "\nAboutToReduce, env before reduction is always as before\n";
      (* print_env env; *)
       (* At this point, we recall that the prefix of the input has been
           successfully recognized as a nonterminal. *)
       run
         (update_last_reduction input_needed production last_reduction)
         input_needed
         lexer
         (resume checkpoint)
  in
  let checkpoint = expressionEOF lexbuf.lex_curr_p in
  let lexer = LexerF.start in
  run (`FoundNothingAt checkpoint) checkpoint lexer checkpoint

let acceptable_me checkpoint token pos =
   let triple = (token, pos, pos) in
   let checkpoint = offer checkpoint triple in
   match shifts checkpoint with
   | None -> (false, None)
   | Some _env -> (true, Some _env)

let rec fail lexer env =
   let (_, startp, endp) = LexerF.get lexer in
   Printf.printf "Error: startp.pos_cnum: %d, endp.pos_cnum: %d\n" startp.pos_cnum endp.pos_cnum;
   match current_state_number env with
   | 14 ->
      (* for '2+' or '2*3+', we add a fake expression: *)
      (* element item: 3: an expression -> an expression + .an expression *)
      let env_new = feed (T T_FAKEEXPRESSION) startp () endp env in
      Printf.printf "BEFORE:\n";
      print_env env;
      Printf.printf "\nAFTER:\n";
      print_env env_new;
      loop lexer (input_needed env_new)
   | _ -> (
      (* for '(1' or '(1+2' or '(1+2*3' or '((1+2)', we add ')': *)
      match acceptable_me (input_needed env) RPAREN endp with
      | (xxx, Some _env) when xxx ->
         Printf.printf "Can amend\n";
         let env_new = feed (T T_FAKERPAREN) startp () endp _env in
         loop lexer (input_needed env_new)
      | _ -> 
         (* for extra closing parenthesis: *) 
         if LexerF.get' lexer = RPAREN
         then loop lexer (input_needed env)
         else failwith "Other cases")

and loop (lexer: LexerF.t) checkpoint =
   match checkpoint with
   | InputNeeded _env ->
      (* The parser needs a token. Request one from the lexer,
         and offer it to the parser, which will produce a new
         checkpoint. Then, repeat. *)
      let token, lexer = LexerF.next lexer in
      let checkpoint = offer checkpoint token in
      loop lexer checkpoint
   | Shifting _ 
   | AboutToReduce _ ->
      let checkpoint = resume checkpoint in
      loop lexer checkpoint
   | HandlingError _env ->
      fail lexer _env
   | Accepted v ->
      (* The parser has succeeded and produced a semantic value. Print it. *)
      Printf.printf "AST:\n%s\n" (AST.show_expression v);
      checkpoint
   | Rejected ->
      (* The parser rejects this input. This cannot happen, here, because
      we stop as soon as the parser reports [HandlingError]. *)
      assert false

let _main =
   let lexbuf = Lexing.from_channel (open_in Sys.argv.(1)) in 
   LexerF.initialize lexbuf;
   loop LexerF.start (expressionEOF lexbuf.lex_curr_p)
  (* Error.resume_on_error (); *)
  (* parse (Lexing.from_channel (open_in Sys.argv.(1))) *)